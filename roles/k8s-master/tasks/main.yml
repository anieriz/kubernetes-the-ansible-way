- name: ensure directories exist
  file:
    path: "{{ item }}"
    state: directory
  with_items:
    - /etc/kubernetes/config
    - /var/lib/kubernetes

- name: copy certificates
  copy:
    src: "/vagrant/{{ item }}"
    dest: /var/lib/kubernetes
    remote_src: yes
  with_items:
    - certificates/ca.pem
    - certificates/ca-key.pem
    - certificates/kube-apiserver-key.pem
    - certificates/kube-apiserver.pem
    - certificates/service-account-key.pem
    - certificates/service-account.pem
    - encryption/encryption-config.yaml

- name: Set flannel network conf on etcd
  uri:
    client_cert: /var/lib/kubernetes/kube-apiserver.pem
    client_key: /var/lib/kubernetes/kube-apiserver-key.pem
    validate_certs: no
    url: https://master-0.k8s:2379/v2/keys/kubernetes-cluster/network/config
    body:
      'value={"Network": "{{ kubernetes_cluster_cidr }}",
         "SubnetLen": 24,
         "Backend": {
             "Type": "vxlan",
             "VNI": 1
         }
       }'
    status_code: 200,201
    method: PUT
    return_content: yes
  run_once: true

# Also bring Flannel onto the master nodes
- name: Install and configure Flannel networking
  include_role:
    name: flannel
  vars:
    etcd_certfile: "/var/lib/kubernetes/kube-apiserver.pem"
    etcd_keyfile: "/var/lib/kubernetes/kube-apiserver-key.pem"

- name: download master component binaries
  get_url:
    url: "https://storage.googleapis.com/kubernetes-release/release/{{ kubernetes_version }}/bin/linux/amd64/{{ item }}"
    dest: "/usr/local/bin/{{ item }}"
    mode: 0755
  with_items:
    - kube-apiserver
    - kube-controller-manager
    - kube-scheduler
    - kubectl

- name: create api-server config
  template:
    src: kube-apiserver.service.j2
    dest: /etc/systemd/system/kube-apiserver.service

- name: copy kube-controller-manager.kubeconfig
  copy:
    src: /vagrant/configurationfiles/kube-controller-manager.kubeconfig
    dest: /var/lib/kubernetes
    remote_src: yes

- name: template kube-controller-manager.service
  template:
    src: kube-controller-manager.service.j2
    dest: /etc/systemd/system/kube-controller-manager.service

- name: copy kube-scheduler.kubeconfig
  copy:
    src: /vagrant/configurationfiles/kube-scheduler.kubeconfig
    dest: /var/lib/kubernetes
    remote_src: yes

- name: copy kube-scheduler.yaml
  copy:
    src: kube-scheduler.yaml
    dest: /etc/kubernetes/config

- name: copy kube-scheduler.service
  copy:
    src: kube-scheduler.service
    dest: /etc/systemd/system

- name: start master component services
  systemd:
    name: "{{ item }}"
    state: restarted
    enabled: yes
    daemon_reload: yes
  with_items:
    - kube-apiserver
    - kube-controller-manager
    - kube-scheduler

# works without parameter kubeconfig: kubectl get componentstatuses
- name: verify, if the k8s cluster is in Healthy state
  shell: kubectl get componentstatuses --kubeconfig /vagrant/configurationfiles/admin.kubeconfig
  register: result
  until: result.stdout.find("Healthy") != -1
  retries: 5
  delay: 10
  when: inventory_hostname == 'master-2'

- name: Show output of k8s cluster state
  debug:
    msg: "{{ result.stdout_lines }}"
  when: inventory_hostname == 'master-2'

# Apply access roles for kube-apiserver
- name: Copy ClusterRole and ClusterRoleBinding for kube-apiserver accessing kubelets
  copy:
    src: "{{ item }}"
    dest: /etc/kubernetes/config/
  with_items:
  - kube-apiserver-cluster-role.yaml
  - kube-apiserver-cluster-role-binding.yaml
  when: inventory_hostname == "master-2"

- name: Apply ClusterRole for kube-apiserver accessing kubelets
  shell: kubectl apply --kubeconfig /vagrant/configurationfiles/admin.kubeconfig -f /etc/kubernetes/config/kube-apiserver-cluster-role.yaml
  when: inventory_hostname == "master-2"

- name: Apply ClusterRoleBinding for kube-apiserver accessing kubelets
  shell: kubectl apply --kubeconfig /vagrant/configurationfiles/admin.kubeconfig -f /etc/kubernetes/config/kube-apiserver-cluster-role-binding.yaml
  when: inventory_hostname == "master-2"
